---
layout: post
title:  "Why RL Is Important and How It Is Different To Other AI Frameworks (CS 234)"
date:   2024-09-16 10:00:40
research: false
blurb: "A look at an example post using Bay Jekyll theme."
og_image: /assets/img/learning/cs234_lecture1.jpg
---

<img src="{{ "/assets/img/learning/cs234_lecture1.jpg" | absolute_url }}" alt="mode_shape" class="post-pic"/>
<br />
Picture by [New World AI](https://www.newworldai.com/cs234-reinforcement-learning-lectures-stanford-engineering/)

<br />

This is my intermediary overview of RL based on CS234 from Stanford. The first lecture gives a fantastic motivation to study RL and thoroughly explains a few concepts from Sutton and Barto. The book is exceptionally well-written but lacks the perspective of the post-GPT era and this first lecture really bridges all the relevant knowledge gaps. I learned the subtle differences between MPs, MRPs and MDPs as well as the matrix representaiton of the Bellman equation. The set of Markov Processes is additionally motivated by the differences in state representation and the difference between the history vs observations. Most importantly the lecture slides compare how RL differes to RL, UL, IL and AI Planning which is crucial to know if one wants to understand use-cases of RL in 2024.

<br />

[Click here to access my GitHub code](https://github.com/YaroKazakov/RL-phd/blob/main/stanford_cs234/cs234_Lecture1_notes.pdf)

<br />

[Click here to access CS234 Notes](https://web.stanford.edu/class/cs234/slides/lecture1pre.pdf)

<br />