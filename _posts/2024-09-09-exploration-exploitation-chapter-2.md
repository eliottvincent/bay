---
layout: post
title:  "Exploration and Explotation (Multi-armed Bandits) - Chapter 2 - Sutton and Barto"
date:   2024-09-09 10:00:40
self_learning: true
blurb: "A look at an example post using Bay Jekyll theme."
og_image: /assets/img/learning/optimal_actions.png
---

<img src="{{ "/assets/img/learning/optimal_actions.png" | absolute_url }}" alt="mode_shape" class="post-pic"/>
<br />
<br />

This lecture focused on the challenge of balancing exploration and exploitation in reinforcement learning (RL), crucial for maximizing cumulative rewards over time. We examined the multi-armed bandit problem as a simplified RL setting where the agent must learn to choose actions (or "arms") with the best rewards through action-value methods. The greedy algorithm exploits current knowledge but risks missing better actions, while the epsilon-greedy algorithm introduces random exploration to avoid getting stuck in suboptimal choices. We also explored policy gradients, which optimize policies directly, and the Upper Confidence Bound (UCB) algorithm, which balances exploitation with uncertainty-driven exploration. These algorithms provide different strategies for managing the exploration-exploitation trade-off, each offering distinct advantages depending on the problem.

<br />

[Click here to access my GitHub code](https://github.com/YaroKazakov/RL-phd/blob/main/rl_book/code/Chapter2-Multi-armed-Bandits-problems.ipynb)

<br />
