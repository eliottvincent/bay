---
layout: post
title:  "Markov Decision Processes - Chapter 3 - Sutton and Barto"
date:   2024-09-14 10:00:40
self_learning: true
blurb: "A look at an example post using Bay Jekyll theme."
og_image: /assets/img/learning/MDP.png
---

<img src="{{ "/assets/img/learning/MDP.png" | absolute_url }}" alt="mode_shape" class="post-pic"/>
<br />
<br />

In Chapter 3, I explored how Markov Decision Processes (MDPs) extend the framework of bandit problems by incorporating both evaluative feedback and the need for associative actions based on state transitions. The core challenge is balancing immediate rewards against delayed rewards, as actions influence future states and, consequently, future rewards. The dynamics of an MDP are defined by transition probabilities p(s′, r∣ s,a), which fully describe the environment’s behavior under the Markov property. I also examined how the return is computed, either in episodic tasks, where there's a terminal state, or continuous tasks, and the significance of discounting rewards using a factor γ. Furthermore, I worked through exercises to devise MDPs in diverse contexts and explored how cumulative rewards shift under changes in reward structures.

<br />

[Click here to access my GitHub code](https://github.com/YaroKazakov/RL-phd/blob/main/rl_book/chapter_notes/Chapter3_MDP_notes.pdf)

<br />
